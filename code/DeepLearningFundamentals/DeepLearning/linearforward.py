import numpy as np
def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.

    Arguments:
    A -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)

    Returns:
    Z -- the input of the activation function, also called pre-activation parameter 
    cache -- a python tuple containing "A", "W" and "b" ; stored for computing the backward pass efficiently
    """
    
    #(â‰ˆ 1 line of code)
    # Z = ...
    # YOUR CODE STARTS HERE
    Z =  np.dot(W, A) + b
    # YOUR CODE ENDS HERE
    cache = (A, W, b)
    
    return Z, cache